{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "elder-trustee",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "czech-india",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:19:18.357175Z",
     "start_time": "2023-06-06T18:19:18.161232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun  7 02:19:18 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 76%   74C    P2   147W / 260W |   9502MiB / 11019MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      6078      C   ...uo/kuokuo_env/bin/python3     1743MiB |\n",
      "|    0   N/A  N/A      6410      C   ...uo/kuokuo_env/bin/python3     1743MiB |\n",
      "|    0   N/A  N/A      7694      C   ...uo/kuokuo_env/bin/python3     1743MiB |\n",
      "|    0   N/A  N/A     14112      C   .../Steph_C/myenv/bin/python     4269MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "metric-kenya",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:19:18.363542Z",
     "start_time": "2023-06-06T18:19:18.359751Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conceptual-biotechnology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:19:21.634712Z",
     "start_time": "2023-06-06T18:19:18.367185Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-dco8htsj because the default path (/home/emma/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import my_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "connected-melbourne",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:19:21.640247Z",
     "start_time": "2023-06-06T18:19:21.637471Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incoming-rating",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:19:21.713419Z",
     "start_time": "2023-06-06T18:19:21.642367Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "short-maldives",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:19:21.775079Z",
     "start_time": "2023-06-06T18:19:21.714515Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bizarre-klein",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:19:21.843099Z",
     "start_time": "2023-06-06T18:19:21.776674Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wired-riding",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:19:21.909930Z",
     "start_time": "2023-06-06T18:19:21.852460Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "communist-satellite",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:19:21.990571Z",
     "start_time": "2023-06-06T18:19:21.912806Z"
    }
   },
   "outputs": [],
   "source": [
    "from AutomaticWeightedLoss import AutomaticWeightedLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "personal-transaction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:19:22.108524Z",
     "start_time": "2023-06-06T18:19:21.992846Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "looking-hindu",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:19:22.351387Z",
     "start_time": "2023-06-06T18:19:22.172700Z"
    }
   },
   "outputs": [],
   "source": [
    "my_utils.set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-adolescent",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stretch-roman",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:19:22.171369Z",
     "start_time": "2023-06-06T18:19:22.109969Z"
    }
   },
   "outputs": [],
   "source": [
    "## Global Variables\n",
    "\n",
    "# DEVICE: GPU\n",
    "DEVICE = torch.device(\"cuda\", 0)\n",
    "\n",
    "# DataFrame 檔案路徑\n",
    "TRAIN_DF_FILE_PATH = \"./data/train_split.csv\"\n",
    "VALID_DF_FILE_PATH = \"./data/valid_split.csv\"\n",
    "TEST_DF_FILE_PATH = \"./data/test_split.csv\"\n",
    "\n",
    "CROSS_VALIDATION_TRAIN_DF_FILE_PATH = \"./data/cross_validation_train_split.csv\"\n",
    "CROSS_VALIDATION_TEST_DF_FILE_PATH = \"./data/cross_validation_test_split.csv\"\n",
    "\n",
    "RESULT_FILE_PATH = f\"./cross_validation_results/cross_validation_result_test3.txt\"\n",
    "TASK_NUM = \"_test3\"\n",
    "\n",
    "# BERT Tokenizer\n",
    "TOKENIZER = BertTokenizer.from_pretrained(\"IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment\")\n",
    "\n",
    "# BERT Model\n",
    "BERT = BertModel.from_pretrained(\"IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment\")\n",
    "BERT.to(DEVICE)\n",
    "\n",
    "# Model Settings\n",
    "EPOCHS = 300\n",
    "PATIENCE = 50\n",
    "# LR_MODEL = 1e-5\n",
    "# BATCH_SIZE = 128\n",
    "### Multi-task Weighting\n",
    "# ALPHA = 0.5\n",
    "# TAU = 0.1\n",
    "########################\n",
    "LABEL_COLUMN = \"Sentiment\"\n",
    "LABEL_MAPPING = {\"neutral\": 0, \"negative\": 1, \"positive\": 2}\n",
    "MAPPING = {'neutral': 0, 'negative': 1, 'positive': 2, \n",
    "           'apology': 0, 'quotation': 1, 'acknowledge': 2, \n",
    "           'thanking': 3, 'question': 4, 'disagreement': 5, \n",
    "           'greeting': 6, 'reject': 7, 'conventional-closing': 8, \n",
    "           'interjection': 9, 'agreement': 10, 'answer': 11, \n",
    "           'comfort': 12, 'command': 13, 'irony': 14, \n",
    "           'statement-non-opinion': 15, 'statement-opinion': 16, 'appreciation': 17, \n",
    "           'other': 18, 'low': 0.0, 'high': 1.0, '語者一': 0, '語者二': 1}\n",
    "# TASKS = [\"main_loss\"]  #  \"future_DA_loss\", \"sentiment_loss\", \"DA_loss\"\n",
    "MODEL_PATH = \"./cross_validation_model.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-suspension",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "large-stanford",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:19:58.371325Z",
     "start_time": "2023-06-06T18:19:58.360269Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, df, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "        self.main_labels = torch.tensor(df.labels.apply(lambda x: x[\"main_label\"]))\n",
    "        self.future_DA_labels = torch.tensor(df.labels.apply(lambda x: x[\"future_DA_label\"]))\n",
    "        self.sentiment_labels = torch.tensor(df.labels.apply(lambda x: x[\"sentiment_labels\"]))\n",
    "        self.DA_labels = torch.tensor(df.labels.apply(lambda x: x[\"DA_labels\"]))\n",
    "        self.speaker1_labels = torch.tensor(df.labels.apply(lambda x: x[\"big_five_labels\"][\"語者一\"]))\n",
    "        self.speaker2_labels = torch.tensor(df.labels.apply(lambda x: x[\"big_five_labels\"][\"語者二\"]))\n",
    "        self.speaking_order = torch.tensor(df.labels.apply(lambda x: x[\"speaking_order\"]))\n",
    " \n",
    "    def __len__(self):\n",
    "        assert self.main_labels.shape[0] == self.embeddings.shape[0]\n",
    "        return self.main_labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embeddings = self.embeddings[idx]\n",
    "        main_labels = self.main_labels[idx]\n",
    "        future_DA_labels = self.future_DA_labels[idx]\n",
    "        sentiment_labels = self.sentiment_labels[idx]\n",
    "        DA_labels = self.DA_labels[idx]\n",
    "        speaker1_labels = self.speaker1_labels[idx]\n",
    "        speaker2_labels = self.speaker2_labels[idx]\n",
    "        speaking_order = self.speaking_order[idx]\n",
    "        return embeddings, main_labels, future_DA_labels, sentiment_labels, DA_labels, speaker1_labels, speaker2_labels, speaking_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "reserved-crawford",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:19:58.597584Z",
     "start_time": "2023-06-06T18:19:58.519551Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()  # head = 2 or 4 or 8\n",
    "        self.speaker_emb = nn.Embedding(2, 768)\n",
    "        self.contextual_attention_layer = nn.MultiheadAttention(768, num_heads=8, batch_first=True, dropout=0.0)  # 0.3 > 0.7\n",
    "#         self.gru_layer = nn.GRU(768, 768, batch_first=True, bidirectional=False)\n",
    "        self.layer_norm = nn.LayerNorm(768)\n",
    "        self.fc = nn.Linear(768, 128)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.last_attention_layer = nn.MultiheadAttention(128, num_heads=4, batch_first=True, dropout=0.0)  # 0.1 > 0.3\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.main_module = nn.Sequential(\n",
    "            nn.Linear(128, len(LABEL_MAPPING)) # LABEL_NUM classes for main task prediction\n",
    "        )\n",
    "        self.future_DA_module = nn.Sequential(\n",
    "            nn.Linear(128, 19),\n",
    "        )\n",
    "        self.sentiment_module = nn.Sequential(\n",
    "            nn.Linear(128, 3),\n",
    "        )\n",
    "        self.DA_module = nn.Sequential(\n",
    "            nn.Linear(128, 19),\n",
    "        )\n",
    "        self.big_five_module = nn.Sequential(\n",
    "            nn.Linear(128, 5),\n",
    "        )\n",
    "        \n",
    "    def forward(self, embeddings, speaking_order):\n",
    "        speaker_embeddings = self.speaker_emb(speaking_order)\n",
    "        embeddings = embeddings + speaker_embeddings\n",
    "        output, _ = self.contextual_attention_layer(embeddings, embeddings, embeddings)  # output shape: (batch_size, seq_length, hidden_dim)\n",
    "#         output, _ = self.gru_layer(embeddings)\n",
    "        output = self.layer_norm(output)\n",
    "        output = self.fc(output)\n",
    "        output = self.relu_1(output)\n",
    "        # Sentiment output\n",
    "        sentiment_output = self.sentiment_module(output)  # output shape: (batch_size, seq_length, num_classes)\n",
    "        # DA output\n",
    "        DA_output = self.DA_module(output)  # output shape: (batch_size, seq_length, num_classes)\n",
    "        # Big five output\n",
    "        speaker1_output = torch.zeros(output.shape[0], output.shape[2]).to(DEVICE)\n",
    "        speaker2_output = torch.zeros(output.shape[0], output.shape[2]).to(DEVICE)\n",
    "        # Average over time steps for Speaker 1 and Speaker 2\n",
    "        for i in range(output.shape[0]):\n",
    "            s1 = torch.mean(output[i, speaking_order[i] == 0, :], dim=0).to(DEVICE)\n",
    "            s2 = torch.mean(output[i, speaking_order[i] == 1, :], dim=0).to(DEVICE)\n",
    "            speaker1_output[i] = s1\n",
    "            speaker2_output[i] = s2\n",
    "        speaker1_output = self.big_five_module(speaker1_output)  # output shape: (batch_size, 5)\n",
    "        speaker2_output = self.big_five_module(speaker2_output)  # output shape: (batch_size, 5)\n",
    "#         1. concatenated_output = torch.cat((pooled_output, last_output), dim=1)\n",
    "#         2. pooled_output, last_output = torch.mean(output, dim=1), output[:, -1, :]\n",
    "#         2. output = pooled_output + last_output\n",
    "        output, _ = self.last_attention_layer(output[:, -1:, :], output, output)\n",
    "        output = output.squeeze(1)\n",
    "        output = self.relu_2(output)\n",
    "        # Future DA output\n",
    "        future_DA_output = self.future_DA_module(output)  # output shape: (batch_size, num_classes)\n",
    "        # Main output\n",
    "        main_output = self.main_module(output)  # output shape: (batch_size, num_classes) for sentiment prediction\n",
    "        return main_output, future_DA_output, sentiment_output, DA_output, speaker1_output, speaker2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "vocal-wichita",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:19:58.758772Z",
     "start_time": "2023-06-06T18:19:58.680328Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(lr_model, batch_size, tasks, alpha, tau, train_df, valid_df, train_embedding, valid_embedding, verbose=True):\n",
    "    \n",
    "    # Training Record\n",
    "    train_loss_dict = {}\n",
    "    train_accuracy_list = []\n",
    "    train_precision_list = []\n",
    "    train_recall_list = []\n",
    "    train_f1_list = []\n",
    "    \n",
    "    # Validation Record\n",
    "    valid_loss_dict = {}\n",
    "    valid_accuracy_list = []\n",
    "    valid_precision_list = []\n",
    "    valid_recall_list = []\n",
    "    valid_f1_list = []\n",
    "    \n",
    "    # Training and Valid Dataset\n",
    "    train = Dataset(df=train_df, embeddings=train_embedding)\n",
    "    valid = Dataset(df=valid_df, embeddings=valid_embedding)\n",
    "    \n",
    "    # Training and Valid DataLoader\n",
    "    train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Model\n",
    "    model = Model()\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # Loss Weights\n",
    "#     if len(tasks) > 1:\n",
    "#         awl = AutomaticWeightedLoss(num=len(tasks))\n",
    "#         awl = awl.to(DEVICE)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer_model = torch.optim.Adam(model.parameters(), lr=lr_model)\n",
    "#     if len(tasks) > 1:\n",
    "#         optimizer_loss = torch.optim.Adam(awl.parameters(), weight_decay=0, lr=lr_loss)\n",
    "    \n",
    "    # LR Scheduler\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer_model, num_warmup_steps=int(total_steps*0.03), num_training_steps=total_steps)\n",
    "\n",
    "    # Dynamic Loss Weight\n",
    "    t_tau = int(len(train_dataloader) * tau)\n",
    "    T = len(train_dataloader)\n",
    "    \n",
    "    # Early Stopping\n",
    "    trigger_times = 0\n",
    "    best_f1 = float(\"-inf\")\n",
    "    \n",
    "    # Training with epochs\n",
    "    for epoch_num in range(EPOCHS):\n",
    "        \n",
    "        # Set the state of the model to \"Training\"\n",
    "        model.train()\n",
    "        \n",
    "        # To save the training result\n",
    "        train_loss = {}\n",
    "        train_true = torch.tensor([])\n",
    "        train_pred = torch.tensor([])\n",
    "        \n",
    "        first_batch_loss_dict = None\n",
    "        \n",
    "        # Training with batches\n",
    "        for t, (train_embeddings, train_main_labels, train_future_DA_labels, train_sentiment_labels, train_DA_labels, train_speaker1_labels, train_speaker2_labels, train_speaking_order) in enumerate(tqdm(train_dataloader, disable=(not verbose))):\n",
    "            \n",
    "            # Current step (Start from 1)\n",
    "            # t = t + 1\n",
    "            \n",
    "            # Feed the data into the model\n",
    "            train_embeddings = train_embeddings.to(DEVICE)\n",
    "            train_main_labels = train_main_labels.to(DEVICE)\n",
    "            train_future_DA_labels = train_future_DA_labels.to(DEVICE)\n",
    "            train_sentiment_labels = train_sentiment_labels.to(DEVICE)\n",
    "            train_DA_labels = train_DA_labels.to(DEVICE)\n",
    "            train_speaker1_labels = train_speaker1_labels.to(DEVICE)\n",
    "            train_speaker2_labels = train_speaker2_labels.to(DEVICE)\n",
    "            train_speaking_order = train_speaking_order.to(DEVICE)\n",
    "            \n",
    "            optimizer_model.zero_grad()\n",
    "#             if len(tasks) > 1:\n",
    "#                 optimizer_loss.zero_grad()\n",
    "            \n",
    "            main_output, future_DA_output, sentiment_output, DA_output, speaker1_output, speaker2_output = model(train_embeddings, train_speaking_order)\n",
    "            \n",
    "            # Reshape sequence output and labels\n",
    "            sentiment_output = sentiment_output.view(-1, 3)\n",
    "            train_sentiment_labels = train_sentiment_labels.view(-1)\n",
    "            \n",
    "            # Reshape sequence output and labels\n",
    "            DA_output = DA_output.view(-1, 19)\n",
    "            train_DA_labels = train_DA_labels.view(-1)\n",
    "            \n",
    "            loss_dict = my_utils.multi_task_loss(\n",
    "                main_output, \n",
    "                future_DA_output,\n",
    "                sentiment_output, \n",
    "                DA_output, \n",
    "                speaker1_output, \n",
    "                speaker2_output, \n",
    "                train_main_labels, \n",
    "                train_future_DA_labels,\n",
    "                train_sentiment_labels, \n",
    "                train_DA_labels, \n",
    "                train_speaker1_labels, \n",
    "                train_speaker2_labels,\n",
    "            )\n",
    "            \n",
    "            if len(tasks) == 1:\n",
    "                loss = loss_dict[\"total_loss\"]\n",
    "            else:\n",
    "                if t == 0:\n",
    "                    first_batch_loss_dict = loss_dict.copy()\n",
    "                mu = t / t_tau if t <= t_tau else 1.0  # (t / T)\n",
    "                losses = [loss_dict[\"main_loss\"],]\n",
    "                weights = [mu,]\n",
    "                for task_name in tasks:\n",
    "                    if task_name == \"main_loss\":\n",
    "                        continue\n",
    "                    else:\n",
    "                        task_loss = loss_dict[task_name]\n",
    "                        first_batch_task_loss = first_batch_loss_dict[task_name]\n",
    "                        lambda_ = (task_loss.item() / first_batch_task_loss.item()) ** alpha\n",
    "                        lambda_ = ((t_tau - t) / t_tau) * lambda_ if t <= t_tau else 0.0\n",
    "                        losses.append(task_loss)\n",
    "                        weights.append(lambda_)\n",
    "                assert len(losses) == len(weights)\n",
    "                losses_tensor = torch.stack(losses).to(DEVICE)\n",
    "                weights_tensor = torch.tensor(weights).to(DEVICE)\n",
    "                weighted_losses = losses_tensor * weights_tensor\n",
    "                loss = torch.sum(weighted_losses)\n",
    "                loss_dict[\"total_loss\"] = loss\n",
    "                    \n",
    "#             else:\n",
    "#                 target_losses = [loss_dict[key] for key in tasks]\n",
    "#                 loss = awl(*target_losses)\n",
    "#                 loss_dict[\"total_loss\"] = loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_model.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "#             if len(tasks) > 1:\n",
    "#                 optimizer_loss.step()\n",
    "            \n",
    "            # Calculate Score\n",
    "            train_main_labels = train_main_labels.cpu()\n",
    "            main_output = torch.argmax(main_output, dim=-1).cpu()\n",
    "            \n",
    "            # Get the results and save them\n",
    "            for loss_type, loss_value in loss_dict.items():\n",
    "                if loss_type not in train_loss:\n",
    "                    train_loss[loss_type] = loss_value.item()\n",
    "                else:\n",
    "                    train_loss[loss_type] += loss_value.item()\n",
    "    \n",
    "            train_true = torch.cat([train_true, train_main_labels])\n",
    "            train_pred = torch.cat([train_pred, main_output])\n",
    "            \n",
    "        # Calculate Metrics\n",
    "        train_accuracy = accuracy_score(train_true, train_pred)\n",
    "        train_precision = precision_score(train_true, train_pred, average='macro')\n",
    "        train_recall = recall_score(train_true, train_pred, average='macro')\n",
    "        train_f1 = f1_score(train_true, train_pred, average='macro')\n",
    "        \n",
    "        for loss_type, loss_value in train_loss.items():\n",
    "            train_loss[loss_type] = loss_value / len(train_dataloader)\n",
    "             \n",
    "        # cal avg train loss and acc\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "        train_precision_list.append(train_precision)\n",
    "        train_recall_list.append(train_recall)\n",
    "        train_f1_list.append(train_f1)\n",
    "        \n",
    "        for loss_type, loss_value in train_loss.items():\n",
    "            if loss_type not in train_loss_dict:\n",
    "                train_loss_dict[loss_type] = [loss_value,]\n",
    "            else:\n",
    "                train_loss_dict[loss_type].append(loss_value)\n",
    "        \n",
    "        # ===============================================================================\n",
    "        \n",
    "        # Set the state of the model to \"Evaluation\"\n",
    "        model.eval()\n",
    "        \n",
    "        # To save the validation result\n",
    "        valid_loss = {}\n",
    "        valid_true = torch.tensor([])\n",
    "        valid_pred = torch.tensor([])\n",
    "        \n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Validation with batches\n",
    "            for valid_embeddings, valid_main_labels, valid_future_DA_labels, valid_sentiment_labels, valid_DA_labels, valid_speaker1_labels, valid_speaker2_labels, valid_speaking_order in tqdm(valid_dataloader, disable=(not verbose)):\n",
    "                \n",
    "                # Feed the data into the model\n",
    "                valid_embeddings = valid_embeddings.to(DEVICE)\n",
    "                valid_main_labels = valid_main_labels.to(DEVICE)\n",
    "                valid_future_DA_labels = valid_future_DA_labels.to(DEVICE)\n",
    "                valid_sentiment_labels = valid_sentiment_labels.to(DEVICE)\n",
    "                valid_DA_labels = valid_DA_labels.to(DEVICE)\n",
    "                valid_speaker1_labels = valid_speaker1_labels.to(DEVICE)\n",
    "                valid_speaker2_labels = valid_speaker2_labels.to(DEVICE)\n",
    "                valid_speaking_order = valid_speaking_order.to(DEVICE)\n",
    "                \n",
    "                main_output, future_DA_output, sentiment_output, DA_output, speaker1_output, speaker2_output = model(valid_embeddings, valid_speaking_order)\n",
    "                \n",
    "                # Reshape sequence output and labels\n",
    "                sentiment_output = sentiment_output.view(-1, 3)\n",
    "                valid_sentiment_labels = valid_sentiment_labels.view(-1)\n",
    "                \n",
    "                # Reshape sequence output and labels\n",
    "                DA_output = DA_output.view(-1, 19)\n",
    "                valid_DA_labels = valid_DA_labels.view(-1)\n",
    "                \n",
    "                loss_dict = my_utils.multi_task_loss(\n",
    "                    main_output, \n",
    "                    future_DA_output,\n",
    "                    sentiment_output, \n",
    "                    DA_output, \n",
    "                    speaker1_output, \n",
    "                    speaker2_output, \n",
    "                    valid_main_labels,\n",
    "                    valid_future_DA_labels,\n",
    "                    valid_sentiment_labels, \n",
    "                    valid_DA_labels, \n",
    "                    valid_speaker1_labels, \n",
    "                    valid_speaker2_labels,\n",
    "                )\n",
    "                \n",
    "                if len(tasks) == 1:\n",
    "                    loss = loss_dict[\"total_loss\"]\n",
    "#                 else:\n",
    "#                     target_losses = [loss_dict[key] for key in tasks]\n",
    "#                     loss = awl(*target_losses)\n",
    "#                     loss_dict[\"total_loss\"] = loss\n",
    "\n",
    "                # Calculate Score\n",
    "                valid_main_labels = valid_main_labels.cpu()\n",
    "                main_output = torch.argmax(main_output, dim=-1).cpu()\n",
    "\n",
    "                # Get the results and save them\n",
    "                for loss_type, loss_value in loss_dict.items():\n",
    "                    if loss_type not in valid_loss:\n",
    "                        valid_loss[loss_type] = loss_value.item()\n",
    "                    else:\n",
    "                        valid_loss[loss_type] += loss_value.item()\n",
    "                        \n",
    "                valid_true = torch.cat([valid_true, valid_main_labels])\n",
    "                valid_pred = torch.cat([valid_pred, main_output])\n",
    "\n",
    "        # Calculate Metrics\n",
    "        valid_accuracy = accuracy_score(valid_true, valid_pred)\n",
    "        valid_precision = precision_score(valid_true, valid_pred, average='macro')\n",
    "        valid_recall = recall_score(valid_true, valid_pred, average='macro')\n",
    "        valid_f1 = f1_score(valid_true, valid_pred, average='macro')\n",
    "        \n",
    "        for loss_type, loss_value in valid_loss.items():\n",
    "            valid_loss[loss_type] = loss_value / len(valid_dataloader)\n",
    "             \n",
    "        # cal avg val loss and acc\n",
    "        valid_accuracy_list.append(valid_accuracy)\n",
    "        valid_precision_list.append(valid_precision)\n",
    "        valid_recall_list.append(valid_recall)\n",
    "        valid_f1_list.append(valid_f1)\n",
    "        \n",
    "        for loss_type, loss_value in valid_loss.items():\n",
    "            if loss_type not in valid_loss_dict:\n",
    "                valid_loss_dict[loss_type] = [loss_value,]\n",
    "            else:\n",
    "                valid_loss_dict[loss_type].append(loss_value)\n",
    "        \n",
    "        if verbose:\n",
    "            # Print the result of each epoch\n",
    "            print(\n",
    "                f\"Epochs: {epoch_num + 1} \\\n",
    "                | Train Loss: {train_loss['total_loss']: .3f} | Train F1: {train_f1: .3f} | Train Precision: {train_precision: .3f} | Train Recall: {train_recall: .3f} \\\n",
    "                | Valid Loss: {valid_loss['total_loss']: .3f} | Valid F1: {valid_f1: .3f} | Valid Precision: {valid_precision: .3f} | Valid Recall: {valid_recall: .3f}\")\n",
    "            # print(awl.params)\n",
    "        \n",
    "        # Early Stopping\n",
    "        if valid_f1 <= best_f1:\n",
    "            trigger_times += 1\n",
    "            if verbose:\n",
    "                print('Trigger times:', trigger_times)\n",
    "\n",
    "            if trigger_times > PATIENCE:\n",
    "                if verbose:\n",
    "                    print('Early stopping! Start the test process.')\n",
    "                break\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('Trigger times: 0')\n",
    "            trigger_times = 0\n",
    "            best_f1 = valid_f1\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "        \n",
    "    return model, train_precision_list, valid_precision_list, train_recall_list, valid_recall_list, \\\n",
    "            train_f1_list, valid_f1_list, train_loss_dict, valid_loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bottom-wireless",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:57:05.511602Z",
     "start_time": "2023-06-06T18:57:05.498682Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, test_df, test_embedding):\n",
    "\n",
    "    # Test Dataset\n",
    "    test = Dataset(df=test_df, embeddings=test_embedding)\n",
    "    \n",
    "    # Test DataLoader\n",
    "    test_dataloader = DataLoader(test, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Run the model on GPU\n",
    "    model = model.to(DEVICE)\n",
    "        \n",
    "    # Set the state of the model to \"Evaluation\"\n",
    "    model.eval()\n",
    "\n",
    "    # To save the test result\n",
    "    test_true = torch.tensor([])\n",
    "    test_pred = torch.tensor([])\n",
    "\n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Test with batches\n",
    "        for test_embeddings, test_main_labels, _, _, _, _, _, test_speaking_order in tqdm(test_dataloader):\n",
    "\n",
    "            # Feed the data into the model\n",
    "            test_embeddings = test_embeddings.to(DEVICE)\n",
    "            test_main_labels = test_main_labels.to(DEVICE)\n",
    "            test_speaking_order = test_speaking_order.to(DEVICE)\n",
    "            main_output, _, _, _, _, _ = model(test_embeddings, test_speaking_order)\n",
    "            main_output = torch.squeeze(main_output)\n",
    "            \n",
    "            # Calculate Score\n",
    "            test_main_labels = test_main_labels.cpu()\n",
    "            main_output = torch.argmax(main_output, dim=-1).cpu()\n",
    "\n",
    "            # Save the result of each batch\n",
    "            test_true = torch.cat([test_true, test_main_labels])\n",
    "            test_pred = torch.cat([test_pred, main_output])\n",
    "\n",
    "        # Calculate Metrics\n",
    "        test_accuracy = accuracy_score(test_true, test_pred)\n",
    "        test_precision = precision_score(test_true, test_pred, average='macro')\n",
    "        test_recall = recall_score(test_true, test_pred, average='macro')\n",
    "        test_f1 = f1_score(test_true, test_pred, average='macro')\n",
    "        \n",
    "    print(f\"test_accuracy: {round(test_accuracy * 100, 2)}%\")\n",
    "    print(f\"test_precision: {round(test_precision * 100, 2)}%\")\n",
    "    print(f\"test_recall: {round(test_recall * 100, 2)}%\")\n",
    "    print(f\"test_f1: {round(test_f1 * 100, 2)}%\")\n",
    "    \n",
    "    # Draw the result\n",
    "    my_utils.plot_report(test_true, test_pred, list(LABEL_MAPPING.keys()))\n",
    "    my_utils.plot_confusion_matrix(test_true, test_pred, list(LABEL_MAPPING.values()), list(LABEL_MAPPING.keys()))\n",
    "        \n",
    "    return test_accuracy, test_precision, test_recall, test_f1, test_true, test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "concrete-render",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:19:23.766668Z",
     "start_time": "2023-06-06T18:19:23.116676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the Dataset: (94187, 18)\n",
      "The number of dialogues: 8086\n",
      "   TV_ID Dialogue_ID Utterance_ID Speaker  Gender          Age Neuroticism  \\\n",
      "0      1      01_000   01_000_000     童文洁  female  middle-aged        high   \n",
      "1      1      01_000   01_000_001     童文洁  female  middle-aged        high   \n",
      "2      1      01_000   01_000_002      刘静  female  middle-aged         low   \n",
      "3      1      01_000   01_000_003     童文洁  female  middle-aged        high   \n",
      "4      1      01_001   01_001_000      刘静  female  middle-aged         low   \n",
      "\n",
      "  Extraversion Openness Agreeableness Conscientiousness        Scene  \\\n",
      "0         high      low           low              high  other-venue   \n",
      "1         high      low           low              high  other-venue   \n",
      "2         high     high          high              high  other-venue   \n",
      "3         high      low           low              high  other-venue   \n",
      "4         high     high          high              high  other-venue   \n",
      "\n",
      "  FacePosition_LU FacePosition_RD Sentiment  Emotion        DA      Utterance  \n",
      "0         108_136         156_202   neutral  neutral  greeting             真巧  \n",
      "1         193_144         253_197   neutral  neutral  greeting           车没事了  \n",
      "2             0_0             0_0   neutral  neutral    answer  是你呀 没事了没事 谢谢你  \n",
      "3             0_0             0_0   neutral  neutral  question     没事没事 你也去春风  \n",
      "4             0_0             0_0   neutral  neutral    answer              对  \n",
      "======================================================================\n",
      "The shape of the Dataset: (11137, 18)\n",
      "The number of dialogues: 934\n",
      "   TV_ID Dialogue_ID Utterance_ID Speaker  Gender    Age Neuroticism  \\\n",
      "0     32      32_000   32_000_000     于春晓  female  young        high   \n",
      "1     32      32_000   32_000_001     于春晓  female  young        high   \n",
      "2     32      32_000   32_000_002      戴娜  female  young         low   \n",
      "3     32      32_000   32_000_003      戴娜  female  young         low   \n",
      "4     32      32_000   32_000_004      戴娜  female  young         low   \n",
      "\n",
      "  Extraversion Openness Agreeableness Conscientiousness Scene FacePosition_LU  \\\n",
      "0         high      low          high              high   car         335_103   \n",
      "1         high      low          high              high   car          331_90   \n",
      "2          low     high           low              high   car         325_114   \n",
      "3          low     high           low              high   car         337_121   \n",
      "4          low     high           low              high   car          165_82   \n",
      "\n",
      "  FacePosition_RD Sentiment     Emotion                     DA    Utterance  \n",
      "0         493_312  negative  astonished               question          怎么了  \n",
      "1         503_285  negative  astonished               question         撞车了吗  \n",
      "2         387_178  negative     depress                 answer         撞你个头  \n",
      "3         388_180  negative     depress  statement-non-opinion  我这堵着车 你倒睡着了  \n",
      "4         381_286  negative     depress               question        好意思吗你  \n",
      "======================================================================\n",
      "The shape of the Dataset: (27438, 18)\n",
      "The number of dialogues: 2815\n",
      "   TV_ID Dialogue_ID Utterance_ID Speaker  Gender    Age Neuroticism  \\\n",
      "0     27      27_000   27_000_000     林宛瑜  female  young         low   \n",
      "1     27      27_000   27_000_001     林宛瑜  female  young         low   \n",
      "2     27      27_000   27_000_002     陆展博  female  young        high   \n",
      "3     27      27_000   27_000_003     林宛瑜  female  young         low   \n",
      "4     27      27_000   27_000_004     林宛瑜  female  young         low   \n",
      "\n",
      "  Extraversion Openness Agreeableness Conscientiousness Scene FacePosition_LU  \\\n",
      "0         high     high          high              high   car          488_14   \n",
      "1         high     high          high              high   car          481_47   \n",
      "2          low     high          high              high   car          129_41   \n",
      "3         high     high          high              high   car          420_29   \n",
      "4         high     high          high              high   car          432_37   \n",
      "\n",
      "  FacePosition_RD Sentiment     Emotion                     DA Utterance  \n",
      "0         654_204   neutral     neutral  statement-non-opinion     好了没事了  \n",
      "1         613_244   neutral     neutral               thanking      谢谢你哦  \n",
      "2         304_264  negative     worried  statement-non-opinion       没关系  \n",
      "3         594_204  negative  astonished           interjection         喂  \n",
      "4         599_234  negative  astonished               question       没事吧  \n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def cross_validation(tasks, tau, alpha, lr_model, batch_size, file_name):\n",
    "    \n",
    "    my_utils.set_seed()\n",
    "    \n",
    "    # File names\n",
    "    true_tensor_file_path = f\"./cross_validation_results/{file_name}_cross_validation_true_tensor.pt\"\n",
    "    pred_tensor_file_path = f\"./cross_validation_results/{file_name}_cross_validation_pred_tensor.pt\"\n",
    "    \n",
    "    # Read Original Dataframe\n",
    "    train_df_original = pd.read_csv(TRAIN_DF_FILE_PATH)\n",
    "    test_df_original = pd.read_csv(TEST_DF_FILE_PATH)\n",
    "    train_test_df = pd.concat([train_df_original, test_df_original], axis=0)\n",
    "    \n",
    "    # K-fold Cross Validation (Split by TV_ID)\n",
    "    k_fold = GroupKFold(n_splits=5)\n",
    "    cross_validation_test_true, cross_validation_test_pred = torch.tensor([]), torch.tensor([])\n",
    "    \n",
    "    print(f\"==============================================\")\n",
    "    print(f\"Cross Validation on Tasks: {tasks}\")\n",
    "    print(f\"----------------------------------------------\")\n",
    "    print(f\"Tau = {tau}; Alpha = {alpha}; LR = {lr_model}; Batch-Size = {batch_size}\")\n",
    "    print(f\"----------------------------------------------\")\n",
    "    \n",
    "    with open(RESULT_FILE_PATH, \"a\") as file:\n",
    "        file.write(f\"==============================================\\n\")\n",
    "        file.write(f\"Cross Validation on Tasks: {tasks}\\n\")\n",
    "        file.write(f\"----------------------------------------------\\n\")\n",
    "        file.write(f\"Tau = {tau}; Alpha = {alpha}; LR = {lr_model}; Batch-Size = {batch_size}\\n\")\n",
    "        file.write(f\"----------------------------------------------\\n\")\n",
    "    \n",
    "    for split, (train_idx, test_idx) in enumerate(k_fold.split(train_test_df.index, groups=train_test_df.TV_ID)):\n",
    "        \n",
    "        my_utils.set_seed()\n",
    "        \n",
    "        train_df_split = train_test_df.iloc[train_idx.tolist()]\n",
    "        test_df_split = train_test_df.iloc[test_idx.tolist()]\n",
    "        train_df_split.to_csv(CROSS_VALIDATION_TRAIN_DF_FILE_PATH, index=False)\n",
    "        test_df_split.to_csv(CROSS_VALIDATION_TEST_DF_FILE_PATH, index=False)\n",
    "        assert len(set(train_df_split.TV_ID.unique()) & set(test_df_split.TV_ID.unique())) == 0\n",
    "\n",
    "        train_df = my_utils.load_df(CROSS_VALIDATION_TRAIN_DF_FILE_PATH)\n",
    "        valid_df = my_utils.load_df(VALID_DF_FILE_PATH)\n",
    "        test_df = my_utils.load_df(CROSS_VALIDATION_TEST_DF_FILE_PATH)\n",
    "\n",
    "        train_df, train_mapping = my_utils.get_dialogues_df(train_df, LABEL_COLUMN, MAPPING, MAPPING)\n",
    "        valid_df, valid_mapping = my_utils.get_dialogues_df(valid_df, LABEL_COLUMN, MAPPING, MAPPING)\n",
    "        test_df, test_mapping = my_utils.get_dialogues_df(test_df, LABEL_COLUMN, MAPPING, MAPPING)\n",
    "\n",
    "        # Assert the mappings between all dataset are the same\n",
    "        assert MAPPING == train_mapping == valid_mapping == test_mapping\n",
    "\n",
    "        # 確保所有文本長度一致\n",
    "        assert min([len(x) for x in train_df.context]) == max([len(x) for x in train_df.context]) == 8\n",
    "\n",
    "        print(\"Train Data's Label Distribution:\")\n",
    "        print(train_df.labels.apply(lambda x: x[\"main_label\"]).value_counts())\n",
    "        print(\"=\" * 35)\n",
    "        print(\"Valid Data's Label Distribution:\")\n",
    "        print(valid_df.labels.apply(lambda x: x[\"main_label\"]).value_counts())\n",
    "        print(\"=\" * 35)\n",
    "        print(\"Test Data's Label Distribution:\")\n",
    "        print(test_df.labels.apply(lambda x: x[\"main_label\"]).value_counts())\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        print(f\"Split: {split+1}\")\n",
    "        print(f\"Total data points: {train_df_split.shape[0] + test_df_split.shape[0]}, Each Set: {train_df_split.shape[0]} / {test_df_split.shape[0]}\")\n",
    "\n",
    "        # Comment these lines if we do not need to calculate the embeddings again\n",
    "        train_embedding = my_utils.get_dialogues_embedding(train_df, TOKENIZER, BERT, DEVICE)\n",
    "        valid_embedding = my_utils.get_dialogues_embedding(valid_df, TOKENIZER, BERT, DEVICE)\n",
    "        test_embedding = my_utils.get_dialogues_embedding(test_df, TOKENIZER, BERT, DEVICE)\n",
    "\n",
    "        train(tasks=tasks, tau=tau, alpha=alpha, lr_model=lr_model, batch_size=batch_size, train_df=train_df, valid_df=valid_df, train_embedding=train_embedding, valid_embedding=valid_embedding, verbose=False)\n",
    "        \n",
    "        model = Model()\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        \n",
    "        test_accuracy, test_precision, test_recall, test_f1, test_true, test_pred = test(model, test_df, test_embedding)\n",
    "        \n",
    "        with open(RESULT_FILE_PATH, \"a\") as file:\n",
    "            file.write(f\"Split {split+1}: test_f1 = {round(test_f1 * 100, 2)}%\\n\")\n",
    "            \n",
    "        cross_validation_test_true = torch.cat([cross_validation_test_true, test_true])\n",
    "        cross_validation_test_pred = torch.cat([cross_validation_test_pred, test_pred])\n",
    "    \n",
    "    # Show and save Final Result\n",
    "    test_accuracy = accuracy_score(cross_validation_test_true, cross_validation_test_pred)\n",
    "    test_precision = precision_score(cross_validation_test_true, cross_validation_test_pred, average='macro')\n",
    "    test_recall = recall_score(cross_validation_test_true, cross_validation_test_pred, average='macro')\n",
    "    test_f1 = f1_score(cross_validation_test_true, cross_validation_test_pred, average='macro')\n",
    "        \n",
    "    print(f\"test_accuracy: {round(test_accuracy * 100, 2)}%\")\n",
    "    print(f\"test_precision: {round(test_precision * 100, 2)}%\")\n",
    "    print(f\"test_recall: {round(test_recall * 100, 2)}%\")\n",
    "    print(f\"test_f1: {round(test_f1 * 100, 2)}%\")\n",
    "    \n",
    "    # Draw the result\n",
    "    my_utils.plot_report(cross_validation_test_true, cross_validation_test_pred, list(LABEL_MAPPING.keys()))\n",
    "    my_utils.plot_confusion_matrix(cross_validation_test_true, cross_validation_test_pred, list(LABEL_MAPPING.values()), list(LABEL_MAPPING.keys()))\n",
    "    \n",
    "    with open(RESULT_FILE_PATH, \"a\") as file:\n",
    "        file.write(f\"----------------------------------------------\\n\")\n",
    "        file.write(f\"Final test_accuracy: {round(test_accuracy * 100, 2)}%\\n\")\n",
    "        file.write(f\"Final test_precision: {round(test_precision * 100, 2)}%\\n\")\n",
    "        file.write(f\"Final test_recall: {round(test_recall * 100, 2)}%\\n\")\n",
    "        file.write(f\"Final test_f1: {round(test_f1 * 100, 2)}%\\n\")\n",
    "        file.write(f\"==============================================\\n\\n\")\n",
    "    \n",
    "    torch.save(cross_validation_test_true, true_tensor_file_path)\n",
    "    torch.save(cross_validation_test_pred, pred_tensor_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-allergy",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RESULT_FILE_PATH, \"w\") as file:\n",
    "    file.write(f\"==============================================\\n\")\n",
    "    \n",
    "experiment_1 = {\n",
    "    \"tasks\": [\"main_loss\", ],  #  \"future_DA_loss\", \"sentiment_loss\", \"DA_loss\"\n",
    "    \"tau\": 0.0,\n",
    "    \"alpha\": 0.0,\n",
    "    \"lr_model\": 5e-5,\n",
    "    \"batch_size\": 32,\n",
    "    \"file_name\": f\"single_task{TASK_NUM}\",\n",
    "}\n",
    "\n",
    "experiment_2 = {\n",
    "    \"tasks\": [\"main_loss\", \"DA_loss\"],  #  \"future_DA_loss\", \"sentiment_loss\", \"DA_loss\"\n",
    "    \"tau\": 1.0,\n",
    "    \"alpha\": 0.5,\n",
    "    \"lr_model\": 5e-5,\n",
    "    \"batch_size\": 32,\n",
    "    \"file_name\": f\"da(seq){TASK_NUM}\",\n",
    "}\n",
    "\n",
    "experiment_3 = {\n",
    "    \"tasks\": [\"main_loss\", \"sentiment_loss\"],  #  \"future_DA_loss\", \"sentiment_loss\", \"DA_loss\"\n",
    "    \"tau\": 1.0,\n",
    "    \"alpha\": 0.5,\n",
    "    \"lr_model\": 5e-5,\n",
    "    \"batch_size\": 32,\n",
    "    \"file_name\": f\"sentiment(seq){TASK_NUM}\",\n",
    "}\n",
    "\n",
    "experiment_4 = {\n",
    "    \"tasks\": [\"main_loss\", \"future_DA_loss\"],  #  \"future_DA_loss\", \"sentiment_loss\", \"DA_loss\"\n",
    "    \"tau\": 1.0,\n",
    "    \"alpha\": 0.5,\n",
    "    \"lr_model\": 5e-5,\n",
    "    \"batch_size\": 32,\n",
    "    \"file_name\": f\"da(future){TASK_NUM}\",\n",
    "}\n",
    "\n",
    "experiment_5 = {\n",
    "    \"tasks\": [\"main_loss\", \"future_DA_loss\", \"sentiment_loss\"],  #  \"future_DA_loss\", \"sentiment_loss\", \"DA_loss\"\n",
    "    \"tau\": 1.0,\n",
    "    \"alpha\": 0.5,\n",
    "    \"lr_model\": 5e-5,\n",
    "    \"batch_size\": 32,\n",
    "    \"file_name\": f\"da(future)_sentiment(seq){TASK_NUM}\",\n",
    "}\n",
    "\n",
    "experiment_6 = {\n",
    "    \"tasks\": [\"main_loss\", \"future_DA_loss\", \"DA_loss\"],  #  \"future_DA_loss\", \"sentiment_loss\", \"DA_loss\"\n",
    "    \"tau\": 1.0,\n",
    "    \"alpha\": 0.5,\n",
    "    \"lr_model\": 5e-5,\n",
    "    \"batch_size\": 32,\n",
    "    \"file_name\": f\"da(future)_da(seq){TASK_NUM}\",\n",
    "}\n",
    "\n",
    "experiment_7 = {\n",
    "    \"tasks\": [\"main_loss\", \"DA_loss\", \"sentiment_loss\"],  #  \"future_DA_loss\", \"sentiment_loss\", \"DA_loss\"\n",
    "    \"tau\": 1.0,\n",
    "    \"alpha\": 0.5,\n",
    "    \"lr_model\": 5e-5,\n",
    "    \"batch_size\": 32,\n",
    "    \"file_name\": f\"da(seq)_sentiment(seq){TASK_NUM}\",\n",
    "}\n",
    "\n",
    "experiment_8 = {\n",
    "    \"tasks\": [\"main_loss\", \"future_DA_loss\", \"DA_loss\", \"sentiment_loss\"],  #  \"future_DA_loss\", \"sentiment_loss\", \"DA_loss\"\n",
    "    \"tau\": 1.0,\n",
    "    \"alpha\": 0.5,\n",
    "    \"lr_model\": 5e-5,\n",
    "    \"batch_size\": 32,\n",
    "    \"file_name\": f\"da(future)_da(seq)_sentiment(seq){TASK_NUM}\",\n",
    "}\n",
    "\n",
    "cross_validation(**experiment_1)\n",
    "cross_validation(**experiment_2)\n",
    "cross_validation(**experiment_3)\n",
    "cross_validation(**experiment_4)\n",
    "cross_validation(**experiment_5)\n",
    "cross_validation(**experiment_6)\n",
    "cross_validation(**experiment_7)\n",
    "cross_validation(**experiment_8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kuokuo_env",
   "language": "python",
   "name": "kuokuo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
